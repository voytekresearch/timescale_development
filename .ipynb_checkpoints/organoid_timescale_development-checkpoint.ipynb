{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import hdf5storage\n",
    "from tqdm.notebook import tqdm\n",
    "from neurodsp.spectral import compute_spectrum\n",
    "from timescales.autoreg import compute_ar_spectrum\n",
    "from timescales.fit import PSD, ACF\n",
    "from ndspflow.workflows import WorkFlow\n",
    "import timescale_development_hf as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '/Users/trevormcpherson/Desktop/PhD/Voytek/Organoids/organoidData/CTC/'\n",
    "#'/Users/blancamartin/Desktop/Voytek_Lab/timescales/organoid_data/CTC_data/'\n",
    "# file type, constant, for each recording there is a directory that is the name of the recording, and then this file name within it\n",
    "file_type = 'LFP_Sp.mat'\n",
    "# all recording folders have this string in their name\n",
    "recording_string = 'CTC'\n",
    "# string to screen out drug manipulation recordings\n",
    "drug_recording_string = 'Drugs'\n",
    "# get recording directories\n",
    "recording_directories = os.listdir(data_dir)\n",
    "# only keep recording directories\n",
    "recording_directories = [s for s in recording_directories if (recording_string in s) & (drug_recording_string not in s)]\n",
    "# number of recordings\n",
    "n_recordings = len(recording_directories)\n",
    "# total number of wells\n",
    "total_n_wells = 12\n",
    "# no data in first 4 wells, if not specifying specific ones to analyze, skip these first 4\n",
    "wells_to_skip = 4\n",
    "# number of wells\n",
    "n_wells = total_n_wells - wells_to_skip\n",
    "# number of channels\n",
    "n_channels = 64\n",
    "# spike recording sample rate\n",
    "spike_sample_rate = 12500 # samp per sec\n",
    "# length of time bins using to separate spikes (seconds)\n",
    "bin_length = 0.05\n",
    "# binned data sample rate\n",
    "bin_sample_rate = 1 / bin_length\n",
    "# bin length in ms\n",
    "bin_length_ms = int(bin_length * 1000)\n",
    "# param for determining network spiking events, floor division of the maximum spikes found in a bin by this number\n",
    "max_spike_floor_division = 2\n",
    "# analysis window kernel (seconds, total of 3)\n",
    "kernel_window_sec = [-0.5,2.5]\n",
    "# length of kernel window\n",
    "kernel_window_length = np.abs(kernel_window_sec[0]) + kernel_window_sec[1]\n",
    "# convert to bins\n",
    "kernel_window_idx = np.divide(kernel_window_sec,bin_length).astype(int)\n",
    "num_bins_kernel = np.abs(kernel_window_idx[0]) + kernel_window_idx[1]\n",
    "# get sequential times of bins in kernel\n",
    "kernel_times = np.linspace(0,kernel_window_length,num_bins_kernel)\n",
    "# day for organoid differentation as a time object\n",
    "day_diff = datetime.strptime(\"081116\",'%m%d%y')\n",
    "# AR spectrum calculation order\n",
    "ar_spec_order = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_flow_dir = '/Users/trevormcpherson/Desktop/PhD/Voytek/Organoids/work_flow_analysis/'\n",
    "metadata_file = work_flow_dir + 'organoid_metadata.pkl'\n",
    "kernel_bins_all_recordings_file = work_flow_dir + 'kernel_bins_all_recordings.pkl'\n",
    "metadata_clean_file = work_flow_dir + 'organoid_metadata_clean.pkl'\n",
    "workflow_file = work_flow_dir + 'workflow_run.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Collect meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata column labels\n",
    "metadata_column_labels = ['recording','day_postdiff','well','channel']\n",
    "# initialise metadata dataframe\n",
    "organoid_metadata = pd.DataFrame(columns=metadata_column_labels, dtype=object)\n",
    "\n",
    "# loop through recordings\n",
    "for this_recording_directory in recording_directories:\n",
    "    # get date string\n",
    "    date_string = this_recording_directory[4:10]\n",
    "    \n",
    "    # convert to date time object\n",
    "    date_object = datetime.strptime(date_string,'%m%d%y')\n",
    "    \n",
    "    #convert data time object to days post differentiation\n",
    "    dfdf = hf.get_dpdf(date_object, day_diff)\n",
    "\n",
    "    # loop through wells\n",
    "    for well_i in range(n_wells):\n",
    "        # loop through channels\n",
    "        for channel_i in range(n_channels):\n",
    "            # build row for matadata dataframe\n",
    "            df_row_list = [date_object, dfdf ,well_i,channel_i]\n",
    "            df_row = pd.DataFrame([df_row_list], columns=metadata_column_labels)\n",
    "            # add row to dataframe\n",
    "            organoid_metadata = pd.concat([organoid_metadata, df_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_file, 'wb') as f:\n",
    "    pickle.dump(organoid_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_file, 'rb') as f:\n",
    "    organoid_metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: # Get network events kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list for storing network event kernels for each recording\n",
    "network_event_kernels_all_recordings = []\n",
    "# initialize list for storing kernel bins for each recording\n",
    "kernel_bins_all_recordings = []\n",
    "# loop through recordings\n",
    "for this_recording_directory in recording_directories:\n",
    "    print(this_recording_directory)\n",
    "    # get mat file path\n",
    "    mat_file_path = data_dir + this_recording_directory + '/' + file_type\n",
    "    # load mat file\n",
    "    data = hdf5storage.loadmat(mat_file_path)\n",
    "    # get spikes\n",
    "    spikes = data['spikes']\n",
    "    # t_s - recording time in seconds\n",
    "    time_sec = data['t_s']\n",
    "    # get last time point in recording\n",
    "    recording_end_time = np.floor(time_sec[-1])\n",
    "    # number of time bins in this recording\n",
    "    num_bins_recording = np.floor(recording_end_time / bin_length).astype('int')\n",
    "\n",
    "    # loop through wells\n",
    "    for well_i in range(wells_to_skip,total_n_wells):\n",
    "        print(well_i)\n",
    "        # get spikes for this well\n",
    "        spikes_well = spikes[well_i]\n",
    "\n",
    "        # initialzie to store all spike timestamps, and binned spikes for channels\n",
    "        spike_times = np.array([])\n",
    "        binned_spikes_all_channels = []\n",
    "        # loop through channels\n",
    "        for channel_i in range(n_channels):\n",
    "            # get spikes for this channel\n",
    "            spikes_channel = spikes_well[channel_i]\n",
    "            # only bin if there are spikes\n",
    "            if spikes_channel.size > 0:\n",
    "                # get spike time stamps (seconds)\n",
    "                spike_times_channel = spikes_channel / spike_sample_rate\n",
    "                # store time stapms for this channel\n",
    "                spike_times = np.concatenate((spike_times_channel.flatten(), spike_times))\n",
    "                # binning function uses ms units, convert spike times to ms\n",
    "                spike_times_channel_ms = spike_times_channel * 1000\n",
    "                # binned spikes\n",
    "                binned_spikes_channel = hf.bin_spikes(spike_times_channel_ms,bin_length_ms,fs=spike_sample_rate,n_recording_bins=num_bins_recording)\n",
    "            else:\n",
    "                binned_spikes_channel = np.array([])\n",
    "            # store binned spikes for this channel\n",
    "            binned_spikes_all_channels.append(binned_spikes_channel)\n",
    "\n",
    "        # binning function uses ms units, convert spike times to ms\n",
    "        spike_times_ms = spike_times * 1000\n",
    "        # bin all spikes together\n",
    "        binned_spikes_all = hf.bin_spikes(spike_times_ms,bin_length_ms,fs=spike_sample_rate,n_recording_bins=num_bins_recording)\n",
    "        # get max number of spikes across all bins\n",
    "        max_bin_spikes = max(binned_spikes_all)\n",
    "        # set threshold for detecting large number of spikes\n",
    "        network_event_thresh = max_bin_spikes // max_spike_floor_division\n",
    "        # get indexes that are greater than threshold\n",
    "        net_event_idxs = np.where(binned_spikes_all > network_event_thresh)[0]\n",
    "        # number of network events\n",
    "        num_net_events = len(net_event_idxs)\n",
    "\n",
    "        # initialize list of total kernels we will be collecting - used to ensure there isnt overlap between kernels\n",
    "        other_kernels = np.zeros((1,2))\n",
    "        # initialize network events we will be storing\n",
    "        network_kernels = np.zeros((1,2))\n",
    "        # initialize and loop through network events\n",
    "        for event in net_event_idxs:\n",
    "            # define windows around each event onset\n",
    "            kernel_start = event + kernel_window_idx[0]\n",
    "            kernel_stop = event + kernel_window_idx[1]\n",
    "            # assume we will store this kernel\n",
    "            store_kernel = 1\n",
    "            # only store if start and stop are both within the range of bins we have\n",
    "            if kernel_start < 0 or kernel_stop > num_bins_recording:\n",
    "                store_kernel = 0\n",
    "            # only store if there is no overlap with previous kernels\n",
    "            for previous_kernel in other_kernels:\n",
    "                if previous_kernel[0] <= kernel_start <= previous_kernel[1] or previous_kernel[0] <= kernel_stop <= previous_kernel[1]:\n",
    "                    store_kernel = 0\n",
    "            # only store if mean firing rate is above 0\n",
    "            net_spikes_this_kernel = binned_spikes_all[kernel_start:kernel_stop]\n",
    "            mean_spiking_this_kernel = np.sum(net_spikes_this_kernel) / num_bins_kernel\n",
    "            if mean_spiking_this_kernel <= 0:\n",
    "                store_kernel = 0\n",
    "            # store unless one of our conditions is not met\n",
    "            if store_kernel:\n",
    "                network_kernels = np.vstack((network_kernels, [kernel_start, kernel_stop])).astype(int)\n",
    "                # also update total list of kernels\n",
    "                other_kernels = np.vstack(\n",
    "                    (other_kernels, [kernel_start, kernel_stop])).astype(int)\n",
    "        # remove initializing zero row\n",
    "        network_kernels = network_kernels[1:]\n",
    "        # number of network kernels\n",
    "        n_network_kernels = len(network_kernels)\n",
    "\n",
    "        # loop through channels\n",
    "        for channel_i in range(n_channels):\n",
    "            # get binned spikes for this channel\n",
    "            binned_spikes_channel = binned_spikes_all_channels[channel_i]\n",
    "            # initialize list for kernels for this channel\n",
    "            kernel_bins_all = []\n",
    "            # loop though network kernels\n",
    "            for kernel_i in range(n_network_kernels):\n",
    "                # get kernel start and stop\n",
    "                kernel_start = network_kernels[kernel_i][0]\n",
    "                kernel_stop = network_kernels[kernel_i][1]\n",
    "                # get bins for this kernel\n",
    "                kernel_bins = binned_spikes_channel[kernel_start:kernel_stop]\n",
    "                # store kernel for this channel\n",
    "                kernel_bins_all.append(kernel_bins)\n",
    "            # store kernel binned data\n",
    "            kernel_bins_all_recordings.append(kernel_bins_all)\n",
    "            # store network event kernels for this well\n",
    "            network_event_kernels_all_recordings.append(network_kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kernel_bins_all_recordings_file, 'wb') as f:\n",
    "    pickle.dump(kernel_bins_all_recordings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(kernel_bins_all_recordings_file, 'rb') as f:\n",
    "    kernel_bins_all_recordings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Remove electrodes with no spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiking_electrode_idxs  = [i for i in range(len(kernel_bins_all_recordings)) if np.array(kernel_bins_all_recordings[i]).size > 0]\n",
    "kernel_bins_all_recordings_clean  = [kernel_bins_all_recordings[i] for i in spiking_electrode_idxs]\n",
    "organoid_metadata_clean = organoid_metadata.iloc[spiking_electrode_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_clean_file, 'wb') as f:\n",
    "    pickle.dump(organoid_metadata_clean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_clean_file, 'rb') as f:\n",
    "    organoid_metadata_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analysis with WorkFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize work flow\n",
    "wf = WorkFlow(y_array=kernel_bins_all_recordings_clean)\n",
    "# set initial fork\n",
    "wf.fork(0)\n",
    "# compute kernel averaged power spectra - welch\n",
    "wf.transform(hf.trial_average_spectrum_welch,bin_sample_rate,f_range=(.001, 200))\n",
    "# spec param fit - welch\n",
    "wf.fit(PSD())\n",
    "# reset fork\n",
    "wf.fork(0)\n",
    "# compute kernel averaged power spectra - ar\n",
    "wf.transform(hf.trial_average_spectrum_ar,bin_sample_rate,ar_spec_order,f_range=(.001,200))\n",
    "# spec param fit - ar\n",
    "wf.fit(PSD())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run work flow\n",
    "wf.run(n_jobs=-1, progress=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(workflow_file, 'wb') as f:\n",
    "    pickle.dump(wf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(workflow_file, 'rb') as f:\n",
    "    wf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "with open(metadata_clean_file, 'rb') as f:\n",
    "    organoid_metadata_clean = pickle.load(f)\n",
    "# load results\n",
    "with open(workflow_file, 'rb') as f:\n",
    "    wf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_freq_ranges = [(0.5,7),(0,7)]\n",
    "spec_types = ['Welch','AR']\n",
    "for spec_type_i,spec_type in enumerate(spec_types):\n",
    "    knee_freq_range = knee_freq_ranges[spec_type_i]\n",
    "    n_fits,n_spec_types = wf.results.shape\n",
    "\n",
    "    taus = [wf.results[i][spec_type_i].tau for i in range(n_fits)]\n",
    "    knee_freqs = [wf.results[i][spec_type_i].knee_freq for i in range(n_fits)]\n",
    "    days = [organoid_metadata_clean['recording'].iloc[i].value for i in range(n_fits)]\n",
    "\n",
    "    plt.figure(); plt.hist(knee_freqs,bins=100); plt.title(spec_type + ' all knee freqs')\n",
    "    plt.figure(); plt.hist(np.log10(knee_freqs),bins=100); plt.title(spec_type + ' all knee freqs log')\n",
    "    plt.figure(); plt.scatter(days,taus); plt.title(spec_type + ' taus over development')\n",
    "    plt.figure(); plt.scatter(days,np.log10(taus)); plt.title(spec_type + ' taus over development log')\n",
    "\n",
    "    unique_recordings = np.unique(organoid_metadata_clean['recording'].values)\n",
    "    unique_wells = np.unique(organoid_metadata_clean['well'].values)\n",
    "    unique_channels = np.unique(organoid_metadata_clean['channel'].values)\n",
    "    n_recordings = len(unique_recordings); n_wells = len(unique_wells); n_channels = len(unique_channels)\n",
    "    compiled_data_taus = np.full((n_recordings,n_wells,n_channels),np.nan)\n",
    "    compiled_data_knee_freqs = np.full((n_recordings,n_wells,n_channels),np.nan)\n",
    "    compiled_data_days = np.full((n_recordings,n_wells,n_channels),np.nan)\n",
    "    for rec_i,recording in enumerate(unique_recordings):\n",
    "        for well_i,well in enumerate(unique_wells):\n",
    "            for channel_i,channel in enumerate(unique_channels):\n",
    "                this_idx_np = np.where((organoid_metadata_clean['recording'].values == recording) & (organoid_metadata_clean['well'].values == well) & (organoid_metadata_clean['channel'].values == channel))[0]\n",
    "                if np.size(this_idx_np) > 0:\n",
    "                    if len(this_idx_np) > 1:\n",
    "                        breakme\n",
    "                    this_idx = this_idx_np[0]\n",
    "                    compiled_data_taus[rec_i,well_i,channel_i] = taus[this_idx]\n",
    "                    compiled_data_knee_freqs[rec_i,well_i,channel_i] = knee_freqs[this_idx]\n",
    "                    compiled_data_days[rec_i,well_i,channel_i] = days[this_idx]\n",
    "\n",
    "    knee_freq_mask = (compiled_data_knee_freqs < knee_freq_range[0]) | (compiled_data_knee_freqs > knee_freq_range[1])\n",
    "    compiled_data_taus_kfm = np.copy(compiled_data_taus); compiled_data_taus_kfm[knee_freq_mask] = np.nan\n",
    "    compiled_data_knee_freqs_kfm = np.copy(compiled_data_knee_freqs); compiled_data_knee_freqs_kfm[knee_freq_mask] = np.nan\n",
    "    compiled_data_days_kfm = np.copy(compiled_data_days); compiled_data_days_kfm[knee_freq_mask] = np.nan\n",
    "\n",
    "    plt.figure(); plt.hist(compiled_data_knee_freqs_kfm.ravel(),bins=100); plt.title(spec_type + ' masked knee freqs')\n",
    "    plt.figure(); plt.hist(np.log10(compiled_data_knee_freqs_kfm.ravel()),bins=100); plt.title(spec_type + ' masked knee freqs log')\n",
    "    plt.figure(); plt.scatter(compiled_data_days_kfm,compiled_data_taus_kfm); plt.title('kfm taus over development')\n",
    "    plt.figure(); plt.scatter(compiled_data_days_kfm,np.log10(compiled_data_taus_kfm)); plt.title(spec_type + ' kfm taus over development log')\n",
    "\n",
    "    compiled_data_taus_log = np.log10(compiled_data_taus)\n",
    "    compiled_data_taus_kmf_log = np.log10(compiled_data_taus_kfm)\n",
    "\n",
    "    compiled_data_taus_chan_mean = np.nanmean(compiled_data_taus,axis=2)\n",
    "    compiled_data_taus_kfm_chan_mean = np.nanmean(compiled_data_taus_kfm,axis=2)\n",
    "    compiled_data_taus_chan_mean_log = np.nanmean(compiled_data_taus_log,axis=2)\n",
    "    compiled_data_taus_kfm_chan_mean_log = np.nanmean(compiled_data_taus_kmf_log,axis=2)\n",
    "\n",
    "    plt.figure(); _ = plt.plot(unique_recordings,compiled_data_taus_chan_mean); plt.title(spec_type + ' chan avg taus over development')\n",
    "    plt.figure(); _ = plt.plot(unique_recordings,compiled_data_taus_chan_mean_log); plt.title(spec_type + ' chan avg taus over development log')\n",
    "    plt.figure(); _ = plt.plot(unique_recordings,compiled_data_taus_kfm_chan_mean); plt.title(spec_type + ' kfm taus over development')\n",
    "    plt.figure(); _ = plt.plot(unique_recordings,compiled_data_taus_kfm_chan_mean_log); plt.title(spec_type + ' kfm taus over development log')\n",
    "\n",
    "    compiled_data_taus_well_mean = np.mean(compiled_data_taus_chan_mean,axis=1)\n",
    "    compiled_data_taus_well_std = np.std(compiled_data_taus_chan_mean,axis=1)\n",
    "    taus_upper_bound = compiled_data_taus_well_mean + compiled_data_taus_well_std\n",
    "    taus_lower_bound = compiled_data_taus_well_mean - compiled_data_taus_well_std\n",
    "\n",
    "    compiled_data_taus_well_mean_log = np.mean(compiled_data_taus_chan_mean_log,axis=1)\n",
    "    compiled_data_taus_well_std_log = np.std(compiled_data_taus_chan_mean_log,axis=1)\n",
    "    taus_upper_bound_log = compiled_data_taus_well_mean_log + compiled_data_taus_well_std_log\n",
    "    taus_lower_bound_log = compiled_data_taus_well_mean_log - compiled_data_taus_well_std_log\n",
    "\n",
    "    compiled_data_taus_kfm_well_mean = np.mean(compiled_data_taus_kfm_chan_mean,axis=1)\n",
    "    compiled_data_taus_kfm_well_std = np.std(compiled_data_taus_kfm_chan_mean,axis=1)\n",
    "    taus_upper_bound_kfm = compiled_data_taus_kfm_well_mean + compiled_data_taus_kfm_well_std\n",
    "    taus_lower_bound_kfm = compiled_data_taus_kfm_well_mean - compiled_data_taus_kfm_well_std\n",
    "\n",
    "    compiled_data_taus_kfm_well_mean_log = np.mean(compiled_data_taus_kfm_chan_mean_log,axis=1)\n",
    "    compiled_data_taus_kfm_well_std_log = np.std(compiled_data_taus_kfm_chan_mean_log,axis=1)\n",
    "    taus_upper_bound_kfm_log = compiled_data_taus_kfm_well_mean_log + compiled_data_taus_kfm_well_std_log\n",
    "    taus_lower_bound_kfm_log = compiled_data_taus_kfm_well_mean_log - compiled_data_taus_kfm_well_std_log\n",
    "\n",
    "    compiled_data_days_kfm_chan_mean = np.nanmean(compiled_data_days_kfm,axis=2)\n",
    "    compiled_data_days_kfm_well_mean = np.nanmean(compiled_data_days_kfm_chan_mean,axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(compiled_data_days_kfm_well_mean,compiled_data_taus_well_mean,c='b',linewidth=2)\n",
    "    plt.fill_between(compiled_data_days_kfm_well_mean,taus_lower_bound,taus_upper_bound,color='b',alpha=0.2)\n",
    "    plt.title(spec_type + ' taus over development')\n",
    "    plt.figure()\n",
    "    plt.plot(compiled_data_days_kfm_well_mean,compiled_data_taus_well_mean_log,c='b',linewidth=2)\n",
    "    plt.fill_between(compiled_data_days_kfm_well_mean,taus_lower_bound_log,taus_upper_bound_log,color='b',alpha=0.2)\n",
    "    plt.title(spec_type + ' taus over development log')\n",
    "    plt.figure()\n",
    "    plt.plot(compiled_data_days_kfm_well_mean,compiled_data_taus_kfm_well_mean,c='b',linewidth=2)\n",
    "    plt.fill_between(compiled_data_days_kfm_well_mean,taus_lower_bound_kfm,taus_upper_bound_kfm,color='b',alpha=0.2)\n",
    "    plt.title(spec_type + ' kfm taus over development')\n",
    "    plt.figure()\n",
    "    plt.plot(compiled_data_days_kfm_well_mean,compiled_data_taus_kfm_well_mean_log,c='b',linewidth=2)\n",
    "    plt.fill_between(compiled_data_days_kfm_well_mean,taus_lower_bound_kfm_log,taus_upper_bound_kfm_log,color='b',alpha=0.2)\n",
    "    plt.title(spec_type + ' kfm taus over development log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c464c225fdcc9707dbfe5f8601ecdb4fece2817f3535537e7ef0601ec8664ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
